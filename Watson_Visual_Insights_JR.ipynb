{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "84a959e7-b0e2-409a-bac2-ea77c6a58c08"
    }
   },
   "source": [
    "# Watson Visual Insights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ee4f1753-7c16-413a-bd0a-dab35dfba4c0"
    }
   },
   "source": [
    "*By J Rogel-Salazar*, *Nov 2016*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbpresent": {
     "id": "e1e2730a-6555-4b25-b55a-36f33b268c76"
    }
   },
   "source": [
    "# Getting Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "abced2b3-e005-4dcb-981d-b7166be680ba"
    }
   },
   "source": [
    "Create an instance of visual insights in your bluemix [dashboard](https://console.ng.bluemix.net/) and save the associated credentials json to creds.json in the same directory as this IPython notebook.\n",
    "\n",
    "In this case I am using Python 3.\n",
    "\n",
    "GitHub [link](https://github.com/watson-developer-cloud/python-sdk) has the code to call the [Visual Watson API](https://github.com/watson-developer-cloud/python-sdk/blob/master/examples/visual_recognition_v3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "796ef4ba-6ade-49ff-b044-117c11d31420"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "# You may need to change to the directoy where \n",
    "# your code lives. If so change the information below\n",
    "# accordingly\n",
    "\n",
    "# os.chdir('~/')\n",
    "# print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "32e1bac0-9ce3-4485-b4d2-9b36a054ff73"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"cred_visual.json\") as f:\n",
    "    creds = json.load(f)['credentials']\n",
    "\n",
    "# You may want to check that the credentials have\n",
    "# been imported correctly:\n",
    "\n",
    "# print(\"Username {api_key} \\nGateway: {url}\".format(**creds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7dac28e7-9267-4753-a0a2-0c4ae005c2c2"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives found\n",
      "Negatives found\n"
     ]
    }
   ],
   "source": [
    "# Making sure that the zip files are in the expected location\n",
    "if not os.path.exists(\"./data/positive.zip\"):\n",
    "    print(\"No positive examples found\")\n",
    "else:\n",
    "    print(\"Positives found\")\n",
    "\n",
    "if not os.path.exists(\"./data/negative.zip\"):\n",
    "    print(\"No negative examples found\")\n",
    "else:\n",
    "    print(\"Negatives found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0fd8516-a0e8-42c6-896e-b1e1ad3b0157"
    }
   },
   "source": [
    "# Call Visual Insights API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "c43b6728-44a6-4d81-813e-9a0c1ce8936d"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<watson_developer_cloud.visual_recognition_v3.VisualRecognitionV3 object at 0x106c92c18>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from os.path import join, dirname\n",
    "from os import environ\n",
    "from watson_developer_cloud import VisualRecognitionV3\n",
    "\n",
    "username = creds[\"api_key\"]\n",
    "psw = creds[\"url\"]\n",
    "\n",
    "visual_recognition = VisualRecognitionV3('2016-07-27', api_key=username)\n",
    "\n",
    "print(visual_recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make sure that the Visual Insights API is working correctly. We will use a picture of Albert Einstein and see the information that Watson provides for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_url = 'https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Einstein](https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg)\n",
    "\n",
    "Here is what Watson tells us about the **classification** of this image: Watson tells us that this is a person... \n",
    "\n",
    "OK... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"custom_classes\": 0,\n",
      "  \"images_processed\": 1,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"resolved_url\": \"https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg\",\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"default\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"person\",\n",
      "              \"type_hierarchy\": \"/people\",\n",
      "              \"score\": 1.0\n",
      "            }\n",
      "          ],\n",
      "          \"name\": \"default\"\n",
      "        }\n",
      "      ],\n",
      "      \"source_url\": \"https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.classify(images_url=test_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the infomration from the **face detection**. Watson tell us that is not a female, gives is a range for the age of the person an the location of the face in the picture. \n",
    "\n",
    "Not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8baddc9f-1426-4e51-b29d-e8cdfa74234b"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"images_processed\": 1,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"resolved_url\": \"https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg\",\n",
      "      \"source_url\": \"https://www.nobelprize.org/nobel_prizes/physics/laureates/1921/einstein.jpg\",\n",
      "      \"faces\": [\n",
      "        {\n",
      "          \"gender\": {\n",
      "            \"gender\": \"FEMALE\",\n",
      "            \"score\": 0.0\n",
      "          },\n",
      "          \"face_location\": {\n",
      "            \"left\": 50,\n",
      "            \"width\": 88,\n",
      "            \"height\": 138,\n",
      "            \"top\": 53\n",
      "          },\n",
      "          \"age\": {\n",
      "            \"max\": 54,\n",
      "            \"min\": 45,\n",
      "            \"score\": 0.348709\n",
      "          }\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(visual_recognition.detect_faces(images_url=test_url), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "44cddde8-d689-443c-a86f-6aeb9612b0fb"
    }
   },
   "source": [
    "# Distinguishing Cats from Dogs (or actually other \"animals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "fbf25830-5eb3-4ba6-836e-e229d67aa0ab"
    }
   },
   "source": [
    "We will use the Watson Visual Insights API to create a classifier that tells us whether the animal in a picture is a cat or not. \n",
    "\n",
    "In this case we are training the classifier with positive examples (cats) versus negative ones (dogs). \n",
    "\n",
    "We use Python to send the requests to the Watson API and use the `create_classifier` method to create a classifier calls 'Dogs vs Cats' with images in the zip files called 'negative.zip' and 'positive.zip'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "a9cb118e-deaa-4e44-bf80-85a10a2d855c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Dogs vs Cats\",\n",
      "  \"status\": \"training\",\n",
      "  \"classifier_id\": \"DogsvsCats_751461720\",\n",
      "  \"owner\": \"d671fed5-b775-4e9f-8f3a-29d90aae8d78\",\n",
      "  \"created\": \"2016-11-17T14:10:21.041Z\",\n",
      "  \"classes\": [\n",
      "    {\n",
      "      \"class\": \"cats\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/negative.zip', 'rb') as dogs, open('./data/positive.zip', 'rb') as cats:\n",
    "    my_classifier = visual_recognition.create_classifier('Dogs vs Cats',\n",
    "                                                         cats_positive_examples=cats,\n",
    "                                                         negative_examples=dogs)\n",
    "    print(json.dumps(my_classifier, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the metadata for the custom classifier we just created. The identifier of the classifier is the 'classifier_id'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "9f0eb161-581a-4182-80e7-bec021135380"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DogsvsCats_751461720\n"
     ]
    }
   ],
   "source": [
    "custom_clf = visual_recognition.list_classifiers()\n",
    "\n",
    "clf_id = custom_clf['classifiers'][0]['classifier_id']\n",
    "\n",
    "print(clf_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plan used for this example we can only have one single custom classifier. If you need a new one, you will need to delete the existing one. This can be done with the `delete_classifier` method. Only run the following line if you need to delete the classfier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "86968ff8-cde2-45b2-933f-5774c593ee51"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# deleting a classifier\n",
    "print(json.dumps(visual_recognition.delete_classifier(classifier_id=clf_id),\n",
    "                 indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "d0eaea57-ecf1-4a7c-b0c0-47a0fdd9caaa"
    }
   },
   "source": [
    "# Telling cats from dogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now use the classifier to tell cats from dogs in test images (not seen by the classifier during training).\n",
    "\n",
    "In this case we have some images in a file called \"mypets.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2a1be99c-2d7d-4abc-859c-a727066c01fe"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"custom_classes\": 1,\n",
      "  \"images_processed\": 7,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [],\n",
      "      \"image\": \"./data/mypets.zip/five.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"DogsvsCats_751461720\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"cats\",\n",
      "              \"score\": 0.554423\n",
      "            }\n",
      "          ],\n",
      "          \"name\": \"Dogs vs Cats\"\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./data/mypets.zip/four.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"DogsvsCats_751461720\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"cats\",\n",
      "              \"score\": 0.573919\n",
      "            }\n",
      "          ],\n",
      "          \"name\": \"Dogs vs Cats\"\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./data/mypets.zip/one.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [],\n",
      "      \"image\": \"./data/mypets.zip/seven.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [],\n",
      "      \"image\": \"./data/mypets.zip/six.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [],\n",
      "      \"image\": \"./data/mypets.zip/three.jpg\"\n",
      "    },\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"DogsvsCats_751461720\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"cats\",\n",
      "              \"score\": 0.541283\n",
      "            }\n",
      "          ],\n",
      "          \"name\": \"Dogs vs Cats\"\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./data/mypets.zip/two.jpg\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/mypets.zip', 'rb') as image_file:\n",
    "    mypets_cf = visual_recognition.classify(images_file=image_file,\\\n",
    "                                                  classifier_ids=[clf_id])\n",
    "    print(json.dumps(mypets_cf, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "8e32af40-ed12-44c7-b750-1e13e3aa015c"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"custom_classes\": 1,\n",
      "  \"images_processed\": 1,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [\n",
      "        {\n",
      "          \"classifier_id\": \"DogsvsCats_751461720\",\n",
      "          \"classes\": [\n",
      "            {\n",
      "              \"class\": \"cats\",\n",
      "              \"score\": 0.573919\n",
      "            }\n",
      "          ],\n",
      "          \"name\": \"Dogs vs Cats\"\n",
      "        }\n",
      "      ],\n",
      "      \"image\": \"./data/one.jpg\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"custom_classes\": 1,\n",
      "  \"images_processed\": 1,\n",
      "  \"images\": [\n",
      "    {\n",
      "      \"classifiers\": [],\n",
      "      \"image\": \"./data/six.jpg\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To classify a single image:\n",
    "\n",
    "with open('./data/one.jpg', 'rb') as image_file:\n",
    "    print(json.dumps(visual_recognition.classify(images_file=image_file, \n",
    "                                                 classifier_ids=[clf_id]), \n",
    "                     indent=2))\n",
    "\n",
    "with open('./data/six.jpg', 'rb') as image_file:\n",
    "    print(json.dumps(visual_recognition.classify(images_file=image_file, \n",
    "                                                 classifier_ids=[clf_id]), \n",
    "                     indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the information returned by the Watson API is a nested dictionary with json-ified fields. We can use some functions to get information from the dictionary. \n",
    "\n",
    "Let us build a function that allows us to get the image id and name and provide the score returned by the Watson API classifier and the class (cats or dogs in this case) that the image belongs to. We write the output to a comma-separeted-value file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b6f57336-c557-4b30-9a3f-e0a2f462057d"
    }
   },
   "outputs": [],
   "source": [
    "def ClassifierInfo(Clf, FileName='./data/Classifier.txt', Alt_class='0', sep='\\t',\n",
    "                   verbose=False):\n",
    "\n",
    "    import fileinput\n",
    "    \n",
    "    num_images = Clf['images_processed']\n",
    "    savefl = open(FileName, 'w+')\n",
    "    \n",
    "    myline = ['Image_Id', 'Image_Name', 'Score', \"Class\"]\n",
    "    mystr = sep.join(myline) +'\\n'\n",
    "    savefl.write(mystr)\n",
    "\n",
    "    for i in range(0,num_images):\n",
    "        id = i\n",
    "\n",
    "        imageName = Clf['images'][i]['image']\n",
    "        print(\"Processing \" + imageName)\n",
    "        classifier = Clf['images'][i]['classifiers']\n",
    "\n",
    "        if len(classifier)!=0:\n",
    "            my_score = classifier[0]['classes'][0]['score']\n",
    "            my_class = classifier[0]['classes'][0]['class']\n",
    "\n",
    "        else:\n",
    "            my_score = '1'\n",
    "            my_class = Alt_class\n",
    "\n",
    "        myline = [str(id), imageName, str(my_score), my_class]\n",
    "        mystr = sep.join(myline) +'\\n'\n",
    "        if verbose:\n",
    "            print(mystr)\n",
    "        savefl.write(mystr)\n",
    "    \n",
    "    savefl.truncate()\n",
    "    savefl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "405a3b75-50d3-459d-8d6d-f88e43ce0210"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./data/mypets.zip/five.jpg\n",
      "0,./data/mypets.zip/five.jpg,1,dogs\n",
      "\n",
      "Processing ./data/mypets.zip/four.jpg\n",
      "1,./data/mypets.zip/four.jpg,0.554423,cats\n",
      "\n",
      "Processing ./data/mypets.zip/one.jpg\n",
      "2,./data/mypets.zip/one.jpg,0.573919,cats\n",
      "\n",
      "Processing ./data/mypets.zip/seven.jpg\n",
      "3,./data/mypets.zip/seven.jpg,1,dogs\n",
      "\n",
      "Processing ./data/mypets.zip/six.jpg\n",
      "4,./data/mypets.zip/six.jpg,1,dogs\n",
      "\n",
      "Processing ./data/mypets.zip/three.jpg\n",
      "5,./data/mypets.zip/three.jpg,1,dogs\n",
      "\n",
      "Processing ./data/mypets.zip/two.jpg\n",
      "6,./data/mypets.zip/two.jpg,0.541283,cats\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ClassifierInfo(mypets_cf, Alt_class='dogs', sep=',', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! \n",
    "\n",
    "Enjoy telling cats from dogs in the many images available in the interwebs."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nbpresent": {
   "slides": {},
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
